---
title: "16S_Pregnancy_RawReadsToAnalysis"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())
```

### Download raw reads from Illumina Base Space {web browser}

https://basespace.illumina.com/

  1. Navigate to the projects tab, and click Download Project.
  2. In our case, we ran the same set of 500 samples (including true samples and controls) twice. Raw sequences were downloaded into `/Users/shasta.webb/Desktop/PregnancyPaper2019` for Run 1 and Run 2.
  
**Note:** Raw data are stored in `/work/melin_lab/SSR-16S-2014-2016-Shasta-Webb/WebbRawSequences` on the ARC cluster. To transfer these to your local directory (if you are working on a laptop) use:

```{unix}
scp -r shasta.webb@arc.ucalgary.ca:/work/melin_lab/SSR-16S-2014-2016-Shasta-Webb/WebbRawSequences /Users/shastawebb/Desktop/CapuchinReproductiveMicrobiome/WebbRawSequences
```

### Move files from subdirectories into main directory {command line}

  1. I renamed the Run 1 directory `SSR_16S_Run1` and the Run 2 directory `SSR_16S_Run2`.
  2. Use this command on the command line to move all fastq files from all subdirectories into the main Run1 or Run2 directory. Run the command below when you in the Run1 directory, then again when you are in the Run2 directory.
  
```{unix}
find . -mindepth 2 -type f -print -exec mv {} . \; #using the find commmand, we can dig into all subdirectories, 
                                                   #gather files, and place them the current directory 

#-mindepth means Find the file under root and one level down
#-type f means plaintext file
#-exec means execute the mv from the filenames that are found into the current directory
#The \; is a ; fed to the program (find) by the \ escape preventing it from be handled by the shell (normally would separate commands). The -exec argument interprets everything as a command up to that inserted ; that ends the -exec stuff. Within the -exec stuff an argument of {} means "insert the file name here".
```

  3. Remove the empty directories in the `SSR_16S_Run1` and `SSR_16S_Run2` directories.
  
```{unix}
ls -d */ #this will list only directories in your current directory
rm -r FASTQ_Generation_2018-11-21_17_25_24Z-138728638/ #removes the empty directory and any remaining contents (should only be other empty directories)
ls | wc -l #this will output how many files are now in the your directory; for this dataset, it should be 1000
```
  
### Merge contents of Run1 and Run2 files {python}

Because we duplicated the MiSeq 2x250 run, we have 2 fastq files for each forward read and each reverse read for 500 samples for a total of 2000 files. We need to concatenate the raw reads for each sample, keeping the forward reads and raw reads separate.

Before the python script I wrote can concatenate the files, we need to unzip them. 

```{unix}
gunzip *.fastq.gz #run this for the files in the Run1 and Run2 directories
```

We can use a for loop in unix to concatenate files with the same name, and move them into a combined folder. In the code below, which we run from the directory WebbRawSequences, we will take all files from SSR_16S_Run1, check if there is an identically named file in SSR_16S_Run2, and if there is, then concatenate them into a CombinedRawReads directory.

```{unix}
for file in SSR_16S_Run1/*; do
   otherfile="$(basename "$file")"
   if [[ -r SSR_16S_Run2/"${otherfile}" ]]; then
       cat "$file" SSR_16S_Run2/"$otherfile" >> CombinedRawReads/"$otherfile"
   fi
done
```

Check to see if the concatenation worked by checking the number of lines in the Run1, Run2, and Combined files.

```{unix}
shastawebb@Shastas-MacBook-Pro SSR_16S_Run1 % wc -l 388-Gaston_S209_L001_R1_001.fastq 
  119636 388-Gaston_S209_L001_R1_001.fastq
shastawebb@Shastas-MacBook-Pro SSR_16S_Run1 % cd ../SSR_16S_Run2 
shastawebb@Shastas-MacBook-Pro SSR_16S_Run2 % wc -l 388-Gaston_S209_L001_R1_001.fastq
  123588 388-Gaston_S209_L001_R1_001.fastq
shastawebb@Shastas-MacBook-Pro SSR_16S_Run2 % cd ../CombinedRawReads 
shastawebb@Shastas-MacBook-Pro CombinedRawReads % wc -l 388-Gaston_S209_L001_R1_001.fastq
  243224 388-Gaston_S209_L001_R1_001.fastq
```

### G-zip files

```{unix}
$ gzip *.fastqf
```

### Inspect raw data using `seqkit` {command line}

Once the Run1 and Run2 fastq files have successfully been concatenated and re-zipped, it's time to inspect the raw reads. To install `seqkit`, first ensure that ANACONDA is installed by typing into the command line `which conda`. If no file path is provided, install ANACONDA.

Once ANACONDA is installed, you can install `seqkit`. The `seqkit` package is useful for looking at basic stats.

```{unix}
$ conda install -c bioconda seqkit
```

Run `seqkit stats` to look at basic stats. Note: Subset the the files you intend to inspect to save time. 

```{unix}
$ seqkit stats *.fastq.gz #You can subset using wildcards (e.g. seqkit stats *Winky*.fastq.gz)
```

### Using `cutadapt` to remove barcodes and indices {command line}

From the MiSeq, we expect to get demultiplexed data. However, remnants of Illumina barcodes, seqeuncing primers, indices, and adapters can remain. We need to run `cutadapt` on all files to ensure removal of these artifacts. At this stage we are *not* removing the 16S amplification primers (that will come later).

To install `cutadapt`, first ensure that Miniconda/Anaconda is installed by typing into the command line `which conda`. If no file path is provided, install Miniconda/Anaconda Once installed, make sure to configure your channels:

```{unix}
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
```

Now you can install `cutadapt`.

Option 1: 

```{unix}
conda install -c bioconda cutadapt
```

Option 2: 

```{unix}
conda create -n cutadaptenv cutadapt
```

Now that `cutadapt` is installed, we can trim barcodes from our raw reads. 

  1. Navigate to the directory above where the raw reads are.
  2. Create a directory called `NoBarcode`.
  
```{unix}
$ mkdir NoBarcode
```
  
  3. Move back into the raw reads directory.
  4. Locate your sequencing primer, barcode, and Illumina index information, which should be available from the institution that prepared your libraries. In our case, we had 500 unique barcodes. In `cutadapt`, "N" will be interpreted as a wildcard, so the command below will remove remnant sequencing primers.
  5. Run the following code to trim all R1 (forward) reads:

```{unix}
for file in *R1_001.fastq.gz; do cutadapt -a CTGTCTCTTATACACATCTCCGAGCCCACGAGACNNNNNNNNATCTCGTATGCCGTCTTCTGCTTG -a CAAGCAGAAGACGGCATACGAGATNNNNNNNNGTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG -j 0 -o ../NoBarcode/$file.fastq $file; done
```

Note: The sequence of the adapter is given with the `-a` option. If no output file is specified via the `-o` option, then the output is sent to the standard output stream. To automatically detect the number of available cores, use `-j 0` (or `--cores=0`). 

  6. Run the following code for reverse reads:

```{unix}
for file in *R2_001.fastq.gz; do cutadapt -a CTGTCTCTTATACACATCTGACGCTGCCGACGANNNNNNNNGTGTAGATCTCGGTGGTCGCCGTATCATT -a AATGATACGGCGACCACCGAGATCTACACNNNNNNNNTCGTCGGCAGCGTCAGATGTGTATAAGAGACAG -j 0 -o ../NoBarcode/$file.fastq $file; done
```

If you end up with an accidental extra file extension, use the command below to remove it!

```{}
for i in *.fastq.gz.fastq; do mv -- "$i" "${i%.fastq.gz.fastq}.fastq.gz"; done
```

The trimmed files should appear in the `NoBarcode` directory. Use `seqkit stats` again to see if basic sequence stats have changed.

*Summary: So far we have downloaded the raw files and used `cutadapt` to remove sequencing primers, barcodes, and indices.*

### Package installation and loading {R}

Before beginning anything in R, ensure that the following are installed and loaded:

-Bioconductor

```{r}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install()
library(BiocManager)
```

-Biostrings

```{r}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("Biostrings")
library(Biostrings)
```

-dada2

```{r}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("dada2") #version = "3.9")
library(dada2)
```

-phyloseq

```{r}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("phyloseq")
library(phyloseq)
```

-vegan

```{r}
install.packages("vegan")
library(vegan)
```

-DESeq2

```{r}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("DESeq2")
library(DESeq2)
```

-ggplot2

```{r}
install.packages("ggplot2")
library(ggplot2)
```

-grid

```{r}
library(grid)
```

-gridExtra

```{r}
install.packages("gridExtra")
library(gridExtra)
```

-decontam

```{r}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("decontam")
library(decontam)
```

### Loading packages

```{r}
library(Biostrings)
library(dada2)
library(phyloseq)
library(vegan)
library(DESeq2)
library(ggplot2)
library(gridExtra)
library(ShortRead)
```

### Preparing data

The files with barcodes removed should now be in the `NoBarcode` directory. First, we read in the names of the fastq files, and perform some string manipulation. `sort` ensures forward/reverse files are in same order, then we extract sample names from our forward list.

```{r path}
# CHANGE ME to the directory containing the fastq files after unzipping.
setwd("/Users/shastawebb/Desktop/CapuchinReproductiveMicrobiome")
path <- "/Users/shastawebb/Desktop/CapuchinReproductiveMicrobiome/WebbRawSequences/NoBarcode"
length(list.files(path)) #992 files; 2 samples were removed becuase they had duplicate names going into sequencing; 4 additional samples were removed because there were no reverse reads for 302, 391, 419, or 426
```

*Here the string manipulation has to be tailored to each different run.*

```{r names}
fnFs <- sort(list.files(path, pattern="_R1_")) # forward files
length(fnFs) #496, 494 after problem reads and duplicates are removed

fnRs <- sort(list.files(path, pattern="_R2_")) # reverse files
length(fnRs) #496, 494 after problem reads and duplicates are removed

sample.names <- sapply(strsplit(fnFs, "_"), `[`, 1)
sample.names
fnFs <- file.path(path, fnFs)
fnRs <- file.path(path, fnRs)

#These primers were taken from the UMGC document. These are the locus specific primers.

FWD <- "GTGYCAGCMGCCGCGGTA" #forward primer sequence (locus specific primer, NOT indexing primer) 
REV <- "GGACTACHVGGGTWTCTAAT" #reverse primer sequence (locus specific primer, NOT indexing primer)
```

Make a `filtered` subdirectory in `NoBarcodes/`. 

```{r}
filt_path <- file.path(path, "filtered") 
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq"))
length(filtFs) #This should match the number of forward read files you have in the NoBarcode directory. 
length(filtRs) #This should match the number of forward read files you have in the NoBarcode directory. 
```

Here you can check for duplicates before trimming and filtering. 

```{r}
any(duplicated(c(fnFs, fnRs)))
duplicated(c(filtFs, filtRs))

filtFs[duplicated(filtFs)] #this displays exactly which samples are duplicates
```

I found duplicates of 122M. Remove any duplicates at the command line. We do not know if these samples are from the same monkey, so we have to remove them from the data set. *Once you run the chunk below, it is important to run the R chunks above again to ensure the variables reflect the change!*

```{unix}
$ rm JA15145-122M* #remove all files with this pattern in the name
$ ls | grep "JA15145-122M" #check to see if files were removed
```

### Removing locus-specific primers

Next we will be removing primers from all reads. First we need to create a function to get all orientations of the primers. 

```{r}
allOrients <- function(primer) {  # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)      # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}

FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients
```

Before trimming primers or removing low quality reads, we need to remove ambiguous base calls. I removed sample 126M from the dataset and from the metadata. That sample had 2x as many forward reads as reverse reads. 

```{r}
filterAndTrim(fnFs, filtFs, fnRs, filtRs, maxN = 0, multithread = TRUE) #Here we are giving the filterAndTrim function the files from NoBarcodes, removing reads with ambiguous base calls, and putting the filtered files in the filtered/ directory. 
```

Building a primer hits function. 

```{r}
primerHits <- function(primer, fn) {        # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = filtFs[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = filtRs[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = filtFs[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = filtRs[[1]]))
```

The above function detects primers in the reads. In the next step, we will use `cutadapt` again to remove them. 

### Removing primers with `cutadapt`

```{r}
cutadapt <- "/Users/shastawebb/opt/miniconda3/envs/cutadaptenv/bin/cutadapt" # CHANGE ME to the cutadapt path on your machine
system2(cutadapt, args = "--version") # Run shell commands from R

path.cut <- ("/Users/shastawebb/Desktop/CapuchinReproductiveMicrobiome/WebbRawSequences/NoBarcode/filtered")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(sub('\\.fastq.gz.fastq', '.NoPrimer.fq.gz', fnFs)))
fnRs.cut <- file.path(path.cut, basename(sub('\\.fastq.gz.fastq', '.NoPrimer.fq.gz', fnRs)))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)

R1.flags <- paste("-g", FWD, "-a", REV.RC) # Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) # Trim REV and the reverse-complement of FWD off of R2 (reverse reads)

for(i in seq_along(fnFs)) { # Run Cutadapt
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-m", 1,
                             #"-e", 0,
                             #"-O", 17,
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             filtFs[i], filtRs[i])) # input files
}

# Check to see if any primers remain

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))

```

Since no primers were found in any of the reads, we can proceed. 

### Inspect quality profiles

The median quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position (this is more useful for other sequencing technologies, as Illumina reads are typically all the same length, hence the flat red line).

```{r quality forward reads}
 plotQualityProfile(fnFs.cut[1:2]) #inspecting quality of the forward reads
```

The forward reads are good quality. We generally advise trimming the last few nucleotides to avoid less well-controlled errors that can arise there. These quality profiles do not suggest that any additional trimming is needed.

```{r quality R reads}
plotQualityProfile(fnRs.cut[1:2]) #inspecting quality of the reverse reads, which is expected to be worse
```

### Filter and trim

Because of their good quality, we will truncate the forward reads at position 230 (trimming the last 20 nucleotides).

The reverse reads are of significantly worse quality, especially at the end, which is common in Illumina sequencing. This isn’t too worrisome, as DADA2 incorporates quality information into its error model which makes the algorithm robust to lower quality sequence, but trimming as the average qualities crash will improve the algorithm’s sensitivity to rare sequence variants. Based on these profiles, we will truncate the reverse reads at position 200.

*Considerations* Your reads must still overlap after truncation in order to merge them later! Your `truncLen` must be large enough to maintain 20 + nucleotides of overlap between them.

### Filter the reads 

This step usually takes the longest and the most part of your CPU power. Note: In this case we are overwriting the filt files (those that previously only had Ns filtered out. This might be bad practice...)

```{r filter}
help("filterAndTrim") #For more info on filterAndTrim, which is a function within DADA2
out <- filterAndTrim(fnFs.cut, filtFs, fnRs.cut, filtRs, truncLen=c(230,210), maxN=0, maxEE=c(2,5), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE)
head(out) #on the first round, almost 50% of reads were being cut. I will relax maxEE from c(1,1) to c(2,5) and see what happens
saveRDS(out, "truncatedFilteredReads.csv")
```

```{r quality reverse reads}
plotQualityProfile(filtRs[1:2]) #inspecting filtered reverse reads
```

The maxEE parameter sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores. The filtering parameters are starting points, not set in stone. If you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads (eg. maxEE=c(2,5)), and reducing the truncLen to remove low quality tails. Remember though, when choosing truncLen for paired-end reads you must maintain overlap after truncation in order to merge them later.

### Learn the error rates

The DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates.

The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution.

As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

```{r error rates}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
```

The error rates for each possible transition (A→C, A→G, …) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score.

Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected. If everything looks reasonable, we can proceed with confidence.

**Considerations** Parameter learning is computationally intensive, so by default the learnErrors function uses only a subset of the data (the first 100M bases). If you are working with a large dataset and the plotted error model does not look like a good fit, you can try increasing the nbases parameter to see if the fit improves.

### Dereplication

Dereplication combines all identical sequencing reads into into “unique sequences” with a corresponding “abundance” equal to the number of reads with that unique sequence. Dereplication substantially reduces computation time by eliminating redundant comparisons.

Dereplication in the DADA2 pipeline has one crucial addition from other pipelines: DADA2 retains a summary of the quality information associated with each unique sequence. The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads. These quality profiles inform the error model of the subsequent sample inference step, significantly increasing DADA2’s accuracy.

```{r dereplication}
derepFs <- derepFastq(filtFs, verbose=F)
derepRs <- derepFastq(filtRs, verbose=F)

names(derepFs) <- sample.names # Name the derep-class objects by the sample names
names(derepRs) <- sample.names
```

### Inferring seqeunce variants

```{r inference}
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)

dadaFs[[1]] #Inspection
dadaRs[[1]]
```

### Merge paired reads

We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences.

By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region.

```{r merging}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
```

At this point, you will want to save the mergers object. 

```{r save mergers}
saveRDS(mergers, "/Users/shastawebb/Desktop/CapuchinReproductiveMicrobiome/Robjects/mergedReads.RDS")
mergers <- readRDS("/Users/shastawebb/Desktop/CapuchinReproductiveMicrobiome/Robjects/mergedReads.RDS")

head(mergers[[1]]) # Inspect the merger data.frame from the first sample
```

The mergers object is a list of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.

### Construct sequence table 

We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r sequence table}
seqtab <- makeSequenceTable(mergers)
dim(seqtab) #494 22988
table(nchar(getSequences(seqtab))) # Inspect distribution of sequence lengths
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(253,255)] # Remove non-target-length sequences with base R manipulations of the sequence table
table(nchar(getSequences(seqtab2)))
seqtab<-seqtab2
```

### Remove chimeras

The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of the sequence variants after denoising makes identifying chimeras simpler than it is when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r chimeras}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim) #494 18555
sum(seqtab.nochim)/sum(seqtab)
saveRDS(seqtab.nochim, "/Users/shastawebb/Desktop/CapuchinReproductiveMicrobiome/Robjects/seqtab.nochim.RDS")
seqtab.nochim <- readRDS("/Users/shastawebb/Desktop/CapuchinReproductiveMicrobiome/Robjects/seqtab.nochim.RDS")
```

The frequency of chimeric sequences varies substantially from dataset to dataset, and depends on on factors including experimental procedures and sample complexity. Here chimeras make up about 21% of the merged sequence variants, but when we account for the abundances of those variants we see they account for only about 4% of the merged sequence reads.

*Considerations for your own data*

Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to beginning the DADA2 pipeline.

### Track reads through the pipeline

As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline:

```{r track}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

Looks good! We kept the majority of our raw reads, and there is no over-large drop associated with any single step.

*Considerations for your own data*

This is a great place to do a last sanity check. Outside of filtering (depending on how stringent you want to be) there should no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification.

### Assign taxonomy

The DADA2 package provides a native implementation of the naive Bayesian classifier method for this purpose.

The assignTaxonomy function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least minBoot bootstrap confidence. We maintain formatted training fastas for the RDP training set, GreenGenes clustered at 97% identity, and the Silva reference database, and additional trainings fastas suitable for protists and certain specific environments have been contributed.

*Extensions*

The dada2 package also implements a method to make species level assignments based on exact matching between ASVs and sequenced reference strains. Recent analysis suggests that exact matching (or 100% identity) is the only appropriate way to assign species to 16S gene fragments. Currently, species-assignment training fastas are available for the Silva and RDP 16S databases.

These steps are more computationally intensive and best performed on an external machine. I ran these on a Compute Canada cluster. 

First, at the command line, use `scp` to move the seqtab.nochim.RDS file to the remote server.

```{unix}
scp seqtab.nochim.RDS shasta.webb@arc.ucalgary.ca #check to ensure the file was copied
```

Second, we need to get the silva assignment files on the remote machine. Download them to your local machine then scp the files to your remote directory. 

```{unix}
scp silva* wshasta@cedar.computecanada.ca:scratch/16SPregnancyProject
```

In an R script on the remote cluster, assign taxonomy and species. Export the `taxa` object by using `saveRDS`. 

```{r taxonomy}
taxa <- assignTaxonomy(seqtab.nochim, "~/Desktop/CapuchinReproductiveMicrobiome/Silva/silva_nr_v132_train_set.fa", multithread=TRUE)

saveRDS(taxa, "~/Desktop/CapuchinReproductiveMicrobiome/Robjects/taxa.RDS")
taxa <- addSpecies(taxa, "~/Desktop/CapuchinReproductiveMicrobiome/Silva/silva_species_assignment_v132.fa")
```

Once the job on the remote cluter is complete, scp the taxa object to the local machine to continue with analysis in R. 

```{unix}
scp wshasta@cedar.computecanada.ca:scratch/16SPregnancyProject/taxa.RDS ~/Desktop/CapuchinReproductiveMicrobiome/
```

Now use readRDS to save the taxa object in R. 

```{r}
taxa <- readRDS("~/Desktop/CapuchinReproductiveMicrobiome/Robjects/taxa.RDS")
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

### Handoff to Phyloseq package & saving otu table/taxa table for future use

Read in seqtab.nochim if not already in your environment. 

```{r}
seqtab.nochim <- readRDS("~/Desktop/CapuchinReproductiveMicrobiome/Robjects/seqtab.nochim.RDS")
length(rownames(seqtab.nochim)) #494 rows
```

The rownames from your metadata have to match the rownames from your sequencing file.

```{r phyloseq}
SSRmetadata <- read.csv("~/Desktop/CapuchinReproductiveMicrobiome/Metadata/SSRMetaDataContinuous.csv")
SSRmetadata <- as.data.frame(SSRmetadata)
nrow(SSRmetadata) #608 rows
```

For the purposes of this analysis, we are interested in the females only, since we are focusing on how lactation, pregnancy, and cycling states correspond to microbial community structure. The `SSRmetadata` object contains information from all samples, including males and females. To filter out the males, and keep everything else (females and controls) we run the following code.

```{r}
SSRFemalesMetaData <- SSRmetadata %>% filter(!SEX %in% c("Male", "male")); nrow(SSRFemalesMetaData) #443 samples including controls

#remove duplicated 167M sample (result of 2 diff rep statuses); removing mislabeled 123M sample
SSRFemalesMetaData <- SSRFemalesMetaData %>%
                      filter(!SAMPLE_ID %in% c("167M", "JA15145-123M" )) 

row.names(SSRFemalesMetaData) <- SSRFemalesMetaData$SAMPLE_ID #setting the row names for the metadata to be the sample ID
```

Note that we have kept the SSRmetadata dataframe which we can use for extra analysis, or if we need to explore the male samples as well. 

#### Matching metadata to sequence table

If seqtab.nochim still contains male samples or samples of females with unknown rep state, remove them with the line of code below.

```{r data cleaning}
indToRemove <- setdiff(row.names(seqtab.nochim), SSRFemalesMetaData$SAMPLE_ID) ; View(indToRemove)
```

Let's explore mismatches in our metadata and seqtab.nochim. setdiff(x, y) returns the objects that are in x that are not in y. In our case, the above code returns a list of the male sample IDs since they are still in the sequence table. We will capture the output in the malesToRemove variable and subset out seqtab object.

```{r removing males from seqtab}
seqtab.nochim2 <-seqtab.nochim[!rownames(seqtab.nochim) %in% indToRemove, ] ; length(rownames(seqtab.nochim2)) #removing the samples in the list "remove"; 352 rows remain
setdiff(row.names(seqtab.nochim2), SSRFemalesMetaData$SAMPLE_ID) #this should now return 0, since the objects now contain the same sample IDS
seqtab.nochim <- seqtab.nochim2 #saving changes in the original seqtab object
```

Some of our samples were not sequenced, yet still may be represented in the metadata. Use setdiff to compare the samples that are in the metadata still, but are not in the seqtab.nochim. Remove them if present. 

```{r}
setdiff(SSRFemalesMetaData$SAMPLE_ID, row.names(seqtab.nochim)) #this produces a list of females that are in the metadata but NOT in the seqtab

femalesToRemove <- as.character(setdiff(SSRFemalesMetaData$SAMPLE_ID, row.names(seqtab.nochim))) #run the opposite setdiff to ensure that the metadata doesn't contain sample IDs that are not in the seq table.

SSRFemalesMetaData2 <- SSRFemalesMetaData %>%
                       filter(., !(SAMPLE_ID %in% femalesToRemove)) #; View(SSRFemalesMetaData2) #remove the females from the metadata
                      
setdiff(SSRFemalesMetaData2$SAMPLE_ID, row.names(seqtab.nochim)) #confirm the metadata are now matching the seqtab
SSRFemalesMetaData <- SSRFemalesMetaData2 #save changes into the SSRFemalesMetaData df
row.names(SSRFemalesMetaData) <- SSRFemalesMetaData$SAMPLE_ID #reset the sample ID as the rownames for the metadata df
View(SSRFemalesMetaData) #352

saveRDS(seqtab.nochim, "~/Desktop/CapuchinReproductiveMicrobiome/Robjects/seqtab.nochim.filt.RDS")
saveRDS(SSRFemalesMetaData, "~/Desktop/CapuchinReproductiveMicrobiome/Robjects/SSR.metadata.filt.RDS")
```

Reading back in the saved seqtab and metadata

```{r}
seqtab.nochim <- readRDS("~/Desktop/CapuchinReproductiveMicrobiome/Robjects/seqtab.nochim.filt.RDS")
SSRFemalesMetaData <- readRDS("~/Desktop/CapuchinReproductiveMicrobiome/Robjects/SSR.metadata.filt.RDS")
```

Removing positive controls. We extracted mock communities in a dilution series, but unfortunately they were extracted unaccompanied by our samples. The positive samples that we collected from the forest floor near where the fecals came into contact with the ground over-identified samples. Therefore, we are relying on the negative controls we ran during extraction, purification, library prep, and sequencing as our tools to identify contaminants. Further, as the goal of this project was to identify changes in more abundant taxa, very low prevalence taxa--where contaminants are more likely--are not the central topic of investigation here. 

```{r remove positive controls}
posControl <- c("PositivePC1", "PositivePC2", "PositivePC3", "substr369s1", "substr369s2", "substr476s")

seqtab.nochim3 <-seqtab.nochim[!rownames(seqtab.nochim) %in% posControl, ] #; View(rownames(seqtab.nochim3))
seqtab.nochim <- seqtab.nochim3
posToRemove <- as.character(setdiff(SSRFemalesMetaData$SAMPLE_ID, row.names(seqtab.nochim3)))
SSRFemalesMetaData3 <- SSRFemalesMetaData %>%
                       filter(., !(SAMPLE_ID %in% posToRemove)) #; View(SSRFemalesMetaData3)
SSRFemalesMetaData <- SSRFemalesMetaData3
rownames(SSRFemalesMetaData) <- SSRFemalesMetaData$SAMPLE_ID
View(SSRFemalesMetaData) #346 rows
```

Creating a phyloseq object.

```{r}
taxa <- readRDS("~/Desktop/CapuchinReproductiveMicrobiome/Robjects/taxa.RDS")

ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(SSRFemalesMetaData), 
               tax_table(taxa))
ps #18555 taxa and 344 samples

otu_tableGF<-as.matrix(otu_table(seqtab.nochim, taxa_are_rows=FALSE))
saveRDS(otu_tableGF,"~/Desktop/CapuchinReproductiveMicrobiome/Robjects/otuTableGF.RDS")
```

## Microbial ecology: Getting main results

### Running decontam() on a ps object

#### Inspecting library sizes

Be sure your metadata sheet has a column that identifies the sample as control or sample.

```{r}
head(sample_data(ps))
df <- as.data.frame(sample_data(ps)) # Put sample_data into a ggplot-friendly data.frame
df$LibrarySize <- sample_sums(ps)
df <- df[order(df$LibrarySize),]
df$Index <- seq(nrow(df))
ggplot(data=df, aes(x=Index, y=LibrarySize, color=SampleOrControl)) + geom_point()
saveRDS(df, "~/Desktop/CapuchinReproductiveMicrobiome/Robjects/sampleDataDF_16Oct.RDS")
```

#### Identify contaminants - prevalence

In this contaminant identification method we’ll use the “prevalence” method. In this method, the prevalence (presence/absence across samples) of each sequence feature in true positive samples is compared to the prevalence in negative controls to identify contaminants.

```{r}
sample_data(ps)$is.neg <- sample_data(ps)$SampleOrControl == "Control" #creating a column that displays a logical response to if smaple is control or true sample

contamdf.prev <- isContaminant(ps, method="prevalence", neg="is.neg") #running the function to test if seqs are contaminants or not
table(contamdf.prev$contaminant)
head(which(contamdf.prev$contaminant))

contamdf.prev05 <- isContaminant(ps, method="prevalence", neg="is.neg", threshold=0.5) #altering the default threshold
table(contamdf.prev05$contaminant)
```

Let’s take a look at the number of times several of these taxa were observed in negative controls and positive samples.

```{r}
# Make phyloseq object of presence-absence in negative controls and true samples
ps.pa <- transform_sample_counts(ps, function(abund) 1*(abund>0))
ps.pa.neg <- prune_samples(sample_data(ps.pa)$SampleOrControl == "Control", ps.pa)
ps.pa.pos <- prune_samples(sample_data(ps.pa)$SampleOrControl == "Sample", ps.pa)

# Make data.frame of prevalence in positive and negative samples
df.pa <- data.frame(pa.pos=taxa_sums(ps.pa.pos), pa.neg=taxa_sums(ps.pa.neg),
                      contaminant=contamdf.prev05$contaminant)
ggplot(data=df.pa, aes(x=pa.neg, y=pa.pos, color=contaminant)) + geom_point() +
  xlab("Prevalence (Negative Controls)") + ylab("Prevalence (True Samples)")
```

Samples split pretty cleanly into a branch that shows up mostly in positive samples, and another that shows up mostly in negative controls, and the contaminant assignment has done a good job of identifying those mostly in negative controls.

Removing contaminants from the phyloseq object.

```{r}
contamdf.prev05$contamSeq <- rownames(contamdf.prev05) #making the rownames (the sequences) their own column
#contamdf.prev05$contamSeq <- This should now be your contaminant sequences

toKeepdf <- contamdf.prev05 %>% filter(contaminant == "FALSE") #creating a subset dataframe with only the seqs we want to keep (non-contaminants)
toKeepList <- toKeepdf$contamSeq #creating a character vector of those non-contam seqs
ps <- prune_taxa(toKeepList, ps) #removing contaminant taxa 
ps #18544 taxa and 344 samples
```

Let's rerun the contamination detection step to see if it worked.

```{r}
sample_data(ps)$is.neg <- sample_data(ps)$SampleOrControl == "Control"
contamdf.prev05 <- isContaminant(ps, method="prevalence", neg="is.neg", threshold=0.5) #altering the default threshold
table(contamdf.prev05$contaminant) #It has detected no more contaminants
```

Re-running the removal step, if needed.

```{r}
contamdf.prev05$contamSeq <- rownames(contamdf.prev05) #making the rownames (the sequences) their own column
#contamdf.prev05$contamSeq <- This should now be your contaminant sequences

toKeepdf <- contamdf.prev05 %>% filter(contaminant == "FALSE") #creating a subset dataframe with only the seqs we want to keep (non-contaminants)
toKeepList <- toKeepdf$contamSeq #creating a character vector of those non-contam seqs
ps <- prune_taxa(toKeepList, ps) #removing contaminant taxa
ps #18544 taxa and 344 samples
```

Check again to see if it worked.

```{r}
sample_data(ps)$is.neg <- sample_data(ps)$SampleOrControl == "Control"
contamdf.prev05 <- isContaminant(ps, method="prevalence", neg="is.neg", threshold=0.5) #altering the default threshold
table(contamdf.prev05$contaminant) #It has detected 0 more contaminants and we can proceed
```

### Pruning controls from the phyloseq object

```{r}
samplesKeep <- prune_samples(sample_data(ps)$SampleOrControl == "Sample", ps) #removing all but true samples, since controls were already used to decontaminate
ps <- samplesKeep #once we know the samples were pruned, we can save that in our main ps object; note that we now have 335 samples, all of which are from female monkeys
ps #18544 taxa and 332 samples

saveRDS(ps, "~/Desktop/CapuchinReproductiveMicrobiome/Robjects/psDecontam.RDS")
```

### Removing samples with very few reads

Now that we have decontaminated the samples, and filtered so that we now only have female monkeys, let's check to see if any very low read samples remain. 

```{r}
which(!rowSums(otu_table(ps)) > 1000) #this returns the samples that have fewer than 1000 reads
```

Let's remove any samples with fewer than 1000 reads from the ps object.

```{r}
toRemove <- c("JA15145-060M", "JA15145-070M", "JA15145-071M", "212M", "343-Perdita", "362-Abu", "416-Lunalovegood", "475-Ed")
psKeep <- prune_samples(!sample_data(ps)$SAMPLE_ID %in% toRemove, ps)
ps <- psKeep; ps #18544 taxa and 324 samples

saveRDS(ps, "~/Desktop/CapuchinReproductiveMicrobiome/Robjects/psDecontamLowReads.RDS")
```

Let's now remove these same samples from the metadata object, as well as subset it to be samples only, not the negative controls.

```{r}
SSRFemalesMetaData <- SSRFemalesMetaData %>% filter(!SAMPLE_ID %in% toRemove)
SSRF_Samples <- SSRFemalesMetaData %>% filter(SampleOrControl == "Sample")
rownames(SSRF_Samples) <- SSRF_Samples$SAMPLE_ID
```

### Removing uncharacterized phyla

```{r}
ps <- subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "uncharacterized"))
ps #18393 taxa and 324 samples
```

### Removing mitochondria and chloroplasts

```{r}
ps0 <- subset_taxa(ps, !is.na(Class) & !Class %in% c("Chloroplast")) 
ps0 #18097 taxa and 324 samples
ps <- ps0 #saving changes in ps
```

```{r}
ps0 <- subset_taxa(ps, !is.na(Family) & !Family %in% c("Mitochondria")) 
ps0 #14954 taxa and 324 samples
ps <- ps0

saveRDS(ps, "~/Desktop/CapuchinReproductiveMicrobiome/Robjects/psDecontamLowReadsPhylaRemoved.RDS")
```

### Ensuring we only have females for which we have reproductive status data 

```{r}
metadataFilt <- filter(SSRF_Samples, !is.na(ReproductiveStatus)) #pruning samples with no rep state
psState <- prune_samples(!is.na(sample_data(ps)$ReproductiveStatus), ps) #pruning samples with no rep state
psState #14954 taxa and 310 samples

saveRDS(psState, "~/Desktop/CapuchinReproductiveMicrobiome/Robjects/psFilteredWithRepState.RDS")
```

### Alpha-diversity (Shannon) vs. species richness (Chao1)

We are doing this step before filtering for low abundance taxa.

*CHAO1

```{r computation}
adiv<-estimate_richness(psState,measures=c("Observed","Shannon","Chao1"))
metadataFilt$alphadiv<-adiv$Shannon
hist(metadataFilt$alphadiv)
metadataFilt$chao1<-adiv$Chao1
summary(metadataFilt$chao1) #we see a clear break from 400 onward, with the vast majority of samples falling below 400, so we will remove chao1 > 400
hist(metadataFilt$chao1) 
metadataFilt$evenness<-adiv$Shannon/log(adiv$Observed)

metadataFilt <- metadataFilt %>%
                filter(!chao1 > 400)

boxplot(metadataFilt$chao1)

chao1Outliers <- c("JA15145-015M", "456-Toaster", "413-Mim", "380-Mrs-Weasley")

psState1 <- subset_samples(psState, !SAMPLE_ID %in% chao1Outliers) #removing outliers from ps pbject
psState <- psState1 #14954 taxa and 306 samples
```

Boxplots with ggplot2

```{r boxplots rep state}

#Before filtering more, we will save this ps object
saveRDS(psState, "~/Desktop/CapuchinReproductiveMicrobiome/Robjects/psStateFiltered.RDS") #This ps object has all cycling states and stages; it has been decontaminated short read samples have been removed. NA rep statuses have been removed it has not undergone any prevalence filtering; richness outliers have been removed

psState <- readRDS("~/Desktop/CapuchinReproductiveMicrobiome/Robjects/psStateFiltered.RDS")

metadataFilt$ReproductiveStatus <- factor(metadataFilt$ReproductiveStatus, levels = c("Cycling", "Pregnant", "Nursing")) #reording factor levels

write.csv(metadataFilt, "~/Desktop/CapuchinReproductiveMicrobiome/Metadata/metadataFilt_prePrevFiltering.csv")

metadataFilt <- read.csv("~/Desktop/CapuchinReproductiveMicrobiome/Metadata/metadataFilt_prePrevFiltering.csv")

repState_richness <-ggplot(metadataFilt, aes(ReproductiveStatus, chao1,fill=ReproductiveStatus)) +
                           geom_boxplot() +
                           geom_jitter(size = 1, alpha = 0.5) +
                           ylim(0,400) +
                           xlab("") + ylab("\nSpecies Richness (Chao1)\n") +
                           theme_minimal() +
                           scale_fill_manual(values = c("grey35", "lightgrey", "white")) +
                           theme(legend.position = "none"); repState_richness

repState_alphaDiv <-ggplot(metadataFilt, aes(ReproductiveStatus, alphadiv,fill=ReproductiveStatus)) +
                           geom_boxplot() +
                           geom_jitter(size = 1, alpha = 0.5) +
                           ylim(0,6) +
                           xlab("Day in Cycle") + ylab("\nAlpha Diversity (Shannon)\n") +
                           scale_fill_manual(values = c("grey35", "lightgrey", "white")) +
                           xlab("") +
                           theme_minimal() +
                           theme(legend.position = "none"); repState_alphaDiv

SI_figure_StateRichnessAlpha <- ggarrange(repState_richness, repState_alphaDiv,
                                          labels = c("A", "B"),
                                          ncol = 2, nrow = 1, align = "h"); SI_figure_StateRichnessAlpha

ggsave("~/Desktop/CapuchinReproductiveMicrobiome/Figures/StateRichnessAlpha.pdf", plot = SI_figure_StateRichnessAlpha, width = 6, height = 4)
```

### Significant differences among rep states

Linear models, anovas, & Post-hoc tests

```{r linear model}

# Chao1
chao_m1 <- glmer.nb(chao1~
            ReproductiveStatus +
            scale(Rainfall) +
            scale(TemperatureMax) +
            #dietType +
            (1|INDIVIDUAL),
            data=metadataFilt)

summary(chao_m1)
hist(metadataFilt$chao1)
tab_model(chao_m1, string.est = "Estimate")

#Visualizing (incidence rate ratio)
chao_m1_IRR <- plot_model(chao_m1,
       show.values = TRUE,
       vline.color = "Grey") +
       ylim(0.5, 1.5) +
       ylab("\nIncidence Rate Ratios\nChao1 Richness Index") +
       theme_minimal() +
       ggtitle(""); chao_m1_IRR

#Shannon

alpha_m2 <- lmer(alphadiv~
            ReproductiveStatus +
            scale(TemperatureMax) +
            scale(Rainfall) +
            (1|INDIVIDUAL),
            data=metadataFilt)

hist(metadataFilt$alphadiv)

summary(alpha_m2)

tab_model(alpha_m2)

alpha_m2_plot <- plot_model(alpha_m2,
       show.values = TRUE,
       vline.color = "Grey") +
       ylim(-0.25, 0.5) +
       ylab("\nEstimates\nShannon Diversity Index") +
       theme_minimal() +
       ggtitle(""); alpha_m2_plot

```

A useful next step is to explore feature prevalence in the dataset, which we will define here as the number of samples in which a taxa appears at least once. (from f1000 paper)

### Prevalence (states)

A useful next step is to explore feature prevalence in the dataset, which we will define here as the number of samples in which a taxa appears at least once. (from f1000 paper)

```{r}
# Compute prevalence of each feature, store as data.frame
prevdf <- apply(X = otu_table(psState),
                 MARGIN = ifelse(taxa_are_rows(psState), yes = 1, no = 2),
                 FUN = function(x){sum(x > 0)})
# Add taxonomy and total read counts to this data.frame
prevdf <- data.frame(Prevalence = prevdf,
                      TotalAbundance = taxa_sums(psState),
                      tax_table(psState))
```

Are there phyla that are comprised of mostly low-prevalence features? Compute the total and average prevalences of the features in each phylum.

```{r}
plyr::ddply(prevdf, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})
```

```{r}
# Define phyla to filter
filterPhyla = c("Deferribacteres", "Dependentiae", "Entotheonellaeota", "Euryarchaeota", "Fibrobacteres", "Hydrogenedentes", "Lentisphaerae", "Omnitrophicaeota", "Rokubacteria", "Spirochaetes")

# Filter entries with unidentified Phylum.
ps1 <- subset_taxa(psState, !Phylum %in% filterPhyla)
ps1 #14861 taxa and 306 samples
psState <- ps1
```

Prevalence Filtering

```{r}
# Subset to the remaining phyla
prevdf1 <- subset(prevdf, Phylum %in% get_taxa_unique(psState, "Phylum"))
ggplot(prevdf1, aes(TotalAbundance, Prevalence / nsamples(psState),color=Phylum)) +
  # Include a guess for parameter
    geom_hline(yintercept = 0.05, alpha = 0.5, linetype = 2) + 
    geom_point(size = 2, alpha = 0.7) +
    scale_x_log10() +
    xlab("Total Abundance") + 
    ylab("Prevalence [Frac. Samples]") +
    facet_wrap(~Phylum) + 
    theme(legend.position="none")
```

```{r}
#  Define prevalence threshold as 5% of total samples
prevalenceThreshold <- 0.05 * nsamples(psState)
prevalenceThreshold

# Execute prevalence filter, using `prune_taxa()` function
keepTaxa <- rownames(prevdf1)[(prevdf1$Prevalence >= prevalenceThreshold)]
ps2 <- prune_taxa(keepTaxa, psState)
ps2 #312 taxa and 306 samples
psState <- ps2
saveRDS(psState, "~/Desktop/CapuchinReproductiveMicrobiome/Robjects/psStatePrevFilt.RDS")
psState_filt <- readRDS("~/Desktop/CapuchinReproductiveMicrobiome/Robjects/psStatePrevFilt.RDS")
```

### Agglomerate taxa (state)

When there is known to be a lot of species or sub-species functional redundancy in a microbial community, it might be useful to agglomerate the data features corresponding to closely related taxa. Ideally we would know the functional redundancies perfectly ahead of time, in which case we would agglomerate taxa using those defined relationships and the merge_taxa() function in phyloseq. That kind of exquisite functional data is usually not available, and different pairs of microbes will have different sets of overlapping functions, complicating the matter of defining appropriate grouping criteria.

```{r}
# How many genera would be present after filtering?
length(get_taxa_unique(psState_filt, taxonomic.rank = "Genus"))

## [1] 135 genera

psGlom_state <- tax_glom(psState_filt, "Genus", NArm = TRUE)
psGlom_state #134 taxa and 308 samples
```

### Tailoring to different research questions

Abundance value transformation

It is usually necessary to transform microbiome count data to account for differences in library size, variance, scale, etc. The phyloseq package provides a flexible interface for defining new functions to accomplish these transformations of the abundance values via the transform_sample_counts() function. The first argument to this function is the phyloseq object you want to transform, and the second argument is an R function that defines the transformation. The R function is applied sample-wise, expecting that the first unnamed argument is a vector of taxa counts in the same order as the phyloseq object. Additional arguments are passed on to the function specified in the second argument, providing an explicit means to include pre-computed values, previously defined parameters/thresholds, or any other object that might be appropriate for computing the transformed values of interest.

#### Function for plotting abundance according to reproductive state

```{r}
plot_abundance_RepState <- function(physeq,title = "",
			     Facet = "Order", Color = "Phylum"){
  # Arbitrary subset, based on Phylum, for plotting
  p1f <- subset_taxa(physeq, Phylum %in% c("Firmicutes"))
  mphyseq <- psmelt(p1f)
  mphyseq <- subset(mphyseq, Abundance > 0)
  ggplot(data = mphyseq, mapping = aes_string(x = "ReproductiveStatus",y = "Abundance",
                                 color = Color, fill = Color)) +
    geom_violin(fill = NA) +
    geom_point(size = 1, alpha = 0.3,
                position = position_jitter(width = 0.3)) +
    facet_wrap(facets = Facet) + scale_y_log10()+
    theme(legend.position="none")
}
```

The transformation in this case converts the counts from each sample into their frequencies, often referred to as proportions or relative abundances. This function is so simple that it is easiest to define it within the function call to transform_sample_counts().

#### Plotting abundance according to reproductive state

```{r}
# Transform to relative abundance. Save as new object.

psGlomRA_state <- transform_sample_counts(psGlom_state, function(x){x / sum(x)})
#ord.NMDS.bray <- ordinate(ps3ra, method = "NMDS", distance = "bray")
#plot_ordination(ps3ra, ord.NMDS.bray, color = "REP_STATE")

plotBefore <- plot_abundance_RepState(psGlom_state,"")
plotAfter <- plot_abundance_RepState(psGlomRA_state,"")
# Combine each plot into one graphic.
grid.arrange(nrow = 2, plotBefore, plotAfter)
```

#Plotting Bray Curtis Distances

```{r}
sample_data(psState_filt)$ReproductiveStatus <- factor(sample_data(psState_filt)$ReproductiveStatus, levels = c("Cycling", "Pregnant", "Nursing"))

ps.prop <- transform_sample_counts(psState_filt, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")
betaDivPlot <- plot_ordination(ps.prop, ord.nmds.bray, shape="ReproductiveStatus", color = "ReproductiveStatus", title="Bray NMDS"); betaDivPlot

betaDivPlot <- betaDivPlot + 
  theme_minimal() +
  ggtitle("") +
  scale_color_manual(values = c("olivedrab4", "burlywood2", "tan4")) +
  theme(legend.title = element_blank(),
        legend.position = "bottom"); betaDivPlot

metadata <- as(sample_data(psState_filt), "data.frame")

adonis(distance(psState_filt, method="bray") ~ ReproductiveStatus + INDIVIDUAL + scale(Rainfall) ,
       data = metadata)

```

#BarPlots

```{r}
physeq2 = filter_taxa(psState_filt, function(x) mean(x) > 0.1, TRUE)
physeq2
physeq3 = transform_sample_counts(physeq2, function(x) x / sum(x) )
physeq3

psMelt <- psmelt(physeq3)

glom <- tax_glom(physeq3, taxrank = 'Phylum')
glom # should list # taxa as # phyla
data <- psmelt(glom) # create dataframe from phyloseq object
#data$phylum <- as.character(data$phylum) #convert to character

#simple way to rename phyla with < 1% abundance
data$Phylum[data$Abundance < 0.01] <- "< 1% abund."

data <- dplyr::arrange(data, Abundance)

data$ReproductiveStatus <- factor(data$ReproductiveStatus, levels = c("Cycling", "Pregnant", "Nursing"))

data$Phylum <- factor(data$Phylum, levels = c("Firmicutes", "Actinobacteria", "Proteobacteria", "Bacteroidetes", "Epsilonbacteraeota", "Fusobacteria", "Verrucomicrobia", "Tenericutes","Thaumarchaeota","Acidobacteria","Cyanobacteria","Synergistetes","Chlamydiae","< 1% abund."))

stackedPhyla <- ggplot(data = data, aes(x = ReproductiveStatus, y = Abundance)) + 
        geom_bar(aes(fill=Phylum), stat="identity", position="fill") +
        xlab("") + ylab("Relative Abundance") +
        theme_minimal() +
        scale_fill_manual(values = c("khaki4", "khaki3", "khaki2", "bisque3", "darkolivegreen3", "bisque1", "darkseagreen4")) +
        theme(legend.title = element_blank()); stackedPhyla

betaPhylumStack <- ggarrange(betaDivPlot, stackedPhyla, nrow = 1, labels = c("A", "B")); betaPhylumStack

ggsave("~/Desktop/Manuscripts/InProgress/eLifeManuscript/FiguresTables/Webb_eLife_Figure6.pdf", plot = betaPhylumStack, w = 10, h = 5)
      
```

#### Exploring differential abundance (state) phylum

Since we have a variable of interest with >2 levels, and since we are interested in specific changes between different reproductive states, we can use "contrasts" in DESeq2 to explore log fold changes. From the DESeq2 vignette:

"A contrast is a linear combination of estimated log2 fold changes, which can be used to test if differences between groups are equal to zero. The simplest use case for contrasts is an experimental design containing a factor with three levels, say A, B and C. Contrasts enable the user to generate results for all 3 possible differences: log2 fold change of B vs A, of C vs A, and of C vs B. The contrast argument of results function is used to extract test results of log2 fold changes of interest."

```{r}
ps.phylaGlom <- tax_glom(psState, "Phylum", NArm = TRUE)
ps.dSeq <- phyloseq_to_deseq2(ps.phylaGlom, ~ReproductiveStatus) #using agglomerated ps object which includes all taxa except for low prevalence phyla (controls are already filtered out)
ps.dSeq <- estimateSizeFactors(ps.dSeq, geoMeans = apply(counts(ps.dSeq), 1, gm_mean)) #because we have many ASVs that do not appear in all samples, we have to use variance stabilized data to caluculate and visualize differential abundance 
ps.dSeq <- DESeq(ps.dSeq, test="Wald", fitType="local")
plotMA(ps.dSeq)
ps.dSeq <- estimateDispersions(ps.dSeq)
ps.dSeq.vst <- getVarianceStabilizedData(ps.dSeq)
dim(ps.dSeq.vst) #134 308
psVST <- psGlom_state #psVST is now the phyloseq object with the variance transformed counts
otu_table(psVST) <- otu_table(ps.dSeq.vst, taxa_are_rows = TRUE) # psVST variable now has the results of DESeq2 variance-stabilization of counts instead of the original counts.
otu_tableGF<-as.matrix(otu_table(psVST, taxa_are_rows=FALSE))
#View(otu_tableGF)
```

##### Investigating results (ps glom phylum)

```{r}
ps.dSeq <- DESeq(ps.dSeq, test="Wald", fitType="local")
res <- results(ps.dSeq)
plotMA(res)
res <- res[order(res$padj, na.last=NA), ]
alpha <- 0.01
sigtab <- res[(res$padj < alpha), ]
sigtab <- cbind(as(sigtab, "data.frame"), as(tax_table(psGlom_state)[rownames(sigtab), ], "matrix"))
head(sigtab)#no sig differences

sigtabphy1 <- subset(sigtab, !is.na(Phylum))
# Phylum order
x <- tapply(sigtabphy1$log2FoldChange, sigtabphy1$Phylum, function(x) max(x))
x <- sort(x, TRUE)
sigtabphy1$Phylum <- factor(as.character(sigtabphy1$Phylum), levels=names(x))
# Genus order
x <- tapply(sigtabphy1$log2FoldChange, sigtabphy1$Phylum, function(x) max(x))
x <- sort(x, TRUE)
sigtabphy1$Phylum <- factor(as.character(sigtabphy1$Phylum), levels=names(x))

cols <- c("Firmicutes" = "lightcoral", "Actinobacteria" = "#E69F00", "Proteobacteria" = "#56B4E9", "Bacteroidetes" = "blue")

plot1 <- ggplot(sigtabphy1, aes(y=Phylum, x=log2FoldChange, color=Phylum, size = baseMean)) + 
                geom_vline(xintercept = 0.0, color = "gray", size = 0.5) +
                geom_point() + 
                scale_size() +
                xlim(-7, 7) +
                #scale_color_manual(values = cols) +
                theme_minimal() +
                theme(axis.text.x = element_text(angle = -90, hjust = 0, vjust=0.5), 
                      axis.text.y = element_text(face = "italic")) +
                #ggtitle("Cycling to Pregnancy") +
                #guides(size = guide_legend("Base mean")) +
                theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
                      legend.position = "None",
                      axis.title.y = element_blank(),
                      axis.title.x = element_blank()); plot1
```

```{r Preg vs Cyc}
res1 <- results(ps.dSeq, contrast = c("ReproductiveStatus", "Pregnant", "Cycling")) 
plotMA(res1)
res1 <- res[order(res$padj, na.last=NA), ]
alpha <- 0.01
sigtab <- res[(res$padj < alpha), ]
sigtab <- cbind(as(sigtab, "data.frame"), as(tax_table(psVST)[rownames(sigtab), ], "matrix"))
head(sigtab)

sigtabphy1 <- subset(sigtab, !is.na(Phylum))
# Phylum order
x <- tapply(sigtabphy1$log2FoldChange, sigtabphy1$Phylum, function(x) max(x))
x <- sort(x, TRUE)
sigtabphy1$Phylum <- factor(as.character(sigtabphy1$Phylum), levels=names(x))
# Genus order
x <- tapply(sigtabphy1$log2FoldChange, sigtabphy1$Phylum, function(x) max(x))
x <- sort(x, TRUE)
sigtabphy1$Phylum <- factor(as.character(sigtabphy1$Phylum), levels=names(x))

cols <- c("Firmicutes" = "lightcoral", "Actinobacteria" = "#E69F00", "Proteobacteria" = "#56B4E9", "Bacteroidetes" = "blue")

plot1 <- ggplot(sigtabphy1, aes(y=Phylum, x=log2FoldChange, color=Phylum, size = baseMean)) + 
                geom_vline(xintercept = 0.0, color = "gray", size = 0.5) +
                geom_point() + 
                scale_size() +
                #xlim(-1, 1) +
                #scale_color_manual(values = cols) +
                theme_minimal() +
                theme(axis.text.x = element_text(angle = -90, hjust = 0, vjust=0.5), 
                      axis.text.y = element_text(face = "italic")) +
                ggtitle("Cycling to Pregnancy") +
                guides(size = guide_legend("Base mean")) +
                theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
                      legend.position = "None",
                      axis.title.y = element_blank(),
                      axis.title.x = element_blank()); plot1
```

```{r Preg vs nurse}
res2 <- results(ps.dSeq, contrast = c("ReproductiveStatus", "Pregnant", "Nursing")) #NOT significant

res2 <- res2[order(res2$padj, na.last=NA), ]
alpha <- 0.01
sigtab <- res2[(res2$padj < alpha), ]
sigtab <- cbind(as(sigtab, "data.frame"), as(tax_table(psVST)[rownames(sigtab), ], "matrix"))
head(sigtab) #no sig differences

sigtabphy2 <- subset(sigtab, !is.na(Phylum))
# Phylum order
x <- tapply(sigtabphy2$log2FoldChange, sigtabphy2$Phylum, function(x) max(x))
x <- sort(x, TRUE)
sigtabphy2$Phylum <- factor(as.character(sigtabphy2$Phylum), levels=names(x))
# Genus order
x <- tapply(sigtabphy2$log2FoldChange, sigtabphy2$Phylum, function(x) max(x))
x <- sort(x, TRUE)
sigtabphy2$Phylum <- factor(as.character(sigtabphy2$Phylum), levels=names(x))

cols <- c("Firmicutes" = "lightcoral", "Actinobacteria" = "#E69F00", "Proteobacteria" = "#56B4E9", "Bacteroidetes" = "blue")

plot2 <- ggplot(sigtabphy2, aes(y=Phylum, x=log2FoldChange, color=Phylum, size = baseMean)) + 
                geom_vline(xintercept = 0.0, color = "gray", size = 0.5) +
                geom_point() + 
                scale_size() +
                xlim(-7, 7) +
                #scale_color_manual(values = cols) +
                theme_minimal() +
                theme(axis.text.x = element_text(angle = -90, hjust = 0, vjust=0.5), 
                      axis.text.y = element_text(face = "italic")) +
                ggtitle("Pregnancy to Nursing") +
                guides(size = guide_legend("Base mean")) +
                theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
                      legend.position = "None",
                      axis.title.y = element_blank(),
                      axis.title.x = element_blank()); plot2
```

```{r Nurse vs Cyc}
res3 <- results(ps.dSeq, contrast = c("ReproductiveStatus", "Nursing", "Cycling")) #not signficant
res3 <- res3[order(res3$padj, na.last=NA), ]
alpha <- 0.01
sigtab <- res3[(res3$padj < alpha), ]
sigtab <- cbind(as(sigtab, "data.frame"), as(tax_table(psVST)[rownames(sigtab), ], "matrix"))
head(sigtab)

sigtabphy3 <- subset(sigtab, !is.na(Phylum))
# Phylum order
x <- tapply(sigtabphy3$log2FoldChange, sigtabphy3$Phylum, function(x) max(x))
x <- sort(x, TRUE)
sigtabphy3$Phylum <- factor(as.character(sigtabphy3$Phylum), levels=names(x))
# Genus order
x <- tapply(sigtabphy3$log2FoldChange, sigtabphy3$Phylum, function(x) max(x))
x <- sort(x, TRUE)
sigtabphy3$Phylum <- factor(as.character(sigtabphy3$Phylum), levels=names(x))

cols <- c("Firmicutes" = "lightcoral", "Actinobacteria" = "#E69F00", "Proteobacteria" = "#56B4E9", "Bacteroidetes" = "blue")

plot2 <- ggplot(sigtabphy3, aes(y=Phylum, x=log2FoldChange, color=Phylum, size = baseMean)) + 
                geom_vline(xintercept = 0.0, color = "gray", size = 0.5) +
                geom_point() + 
                scale_size() +
                #xlim(-7, 7) +
                #scale_color_manual(values = cols) +
                theme_minimal() +
                theme(axis.text.x = element_text(angle = -90, hjust = 0, vjust=0.5), 
                      axis.text.y = element_text(face = "italic")) +
                ggtitle("Nursing to Cycling") +
                guides(size = guide_legend("Base mean")) +
                theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
                      legend.position = "None",
                      axis.title.y = element_blank(),
                      axis.title.x = element_blank()); plot2
```










